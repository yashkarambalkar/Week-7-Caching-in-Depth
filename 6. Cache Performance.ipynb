{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4e3b0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "builder. \\\n",
    "config('spark.ui.port','0'). \\\n",
    "config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "enableHiveSupport(). \\\n",
    "master('yarn'). \\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1171eae",
   "metadata": {},
   "source": [
    "### Let's create a Parquet format data QUICKLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b59c5177",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = 'order_id long, order_date date, customer_id long, order_status string'\n",
    "orders_df = spark.read. \\\n",
    "format('csv'). \\\n",
    "schema(orders_schema). \\\n",
    "load('/public/trendytech/orders/orders_1gb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b12bb0a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use itv013010_cachingdemo_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df6d1ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.write.saveAsTable(\"itv013010_cachingdemo_db.itv013010_orders_ext1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c00c07",
   "metadata": {},
   "source": [
    "### Can cache lower performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4bdd59",
   "metadata": {},
   "source": [
    "#### Say I've a table which is having data in parquet format...\n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acf40923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|25831125|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from itv013010_cachingdemo_db.itv013010_orders_ext1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9a46ed",
   "metadata": {},
   "source": [
    "### Before cache we get the results faster because it scans only through metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "941b8749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"cache table itv013010_cachingdemo_db.itv013010_orders_ext1 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "978d0ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|25831125|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from itv013010_cachingdemo_db.itv013010_orders_ext1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543c2be5",
   "metadata": {},
   "source": [
    "### After Caching it took more time since it has to go throughout the data and count."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56090b85",
   "metadata": {},
   "source": [
    "#### REMEMBER: CACHE Data will be slower as compared to parquet FILE because it has to go through entire data whereas when scanning data from parquet it will be faster because it reads just metadata.\n",
    "\n",
    "### PARQUET FILE FASTER READS THAN Cached Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70cf5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
